title: Automating the R.E.N.A.L. score in patients with a renal cell carcinoma
groups: diag
closed: true
type: student
picture: vacancies/RCCKidney.jpg
template: vacancy-single
people: Myrthe Buser, Alessa Hering
description: Automating the R.E.N.A.L. score in patients with a renal cell carcinoma

## Clinical Motivation
Renal cell carcinoma (RCC) is the most common form of kidney cancer in adults. Treatment for RCC often involves surgery, either by removing the entire kidney (total nephrectomy) or preserving part of the kidney while removing the tumor (nephron sparing surgery). The choice of surgery is dependent on multiple factors like patient overall health and experience of the treating surgeon, but the main factor is the anatomical complexity of the tumor. 
To assist surgical decision-making, the RENAL score was introduced [1] as a radiological tool to quantify tumor complexity. The RENAL score consists of 5 different components, all focused on different aspects of the tumor (anatomy).
 
Although the RENAL scoring system has been suggested in literature to be useful in surgical planning, the clinical use is limited, mainly because of the time investment of radiologists to obtain all components of the RENAL score and the inter-observer variability between observers. Several efforts have been made to automate the RENAL score, but these methods rely on simplification of the anatomical situation or other heuristics [2], [3]. 

## Research Opportunity
Recent advances in deep learning for medical image analysis now enable the automation of complex anatomical assessments directly from CT scans. At our research group, we develop a suite of AI models that automatically segment relevant structures (e.g., tumors, kidneys, collecting system) and compute features required for the RENAL scoring system. These models, covering all components of the RENAL score, will be available at the start of the project. They provide a strong foundation for further development, refinement, and validation of a more nuanced, continuous, and interpretable version of the RENAL score.

## Objectives / Aims / Goals
**Primary objective**:
Develop and evaluate an enhanced version of the RENAL scoring system using AI-driven image analysis.

**Further goals** (depending on project scope, student interests, literature findings, and initial conclusions):

- Automate and compare the RENAL scoring system to other scoring systems (e.g., C-score or PADUA)
- Add explainability to the scoring system (for example by visualizing the different distance based measures).

## Requirements
- Master students with a major in computer science, artificial intelligence, mathematics, biomedical engineering, technical medicine, physics, or a related area in the final stage of master's studies
- Experience with programming in Python
- Experience in deep learning, medical imaging, and/or medical image analysis

## Practical Information
**Project Duration**: Variable (6 months for MSc thesis), starting September 2025. 

**Location**: Department of Medical Imaging, Radboud University Medical Center, Nijmegen

**For more information**, please contact Myrthe Buser or Alessa Hering. 

**Application**: Please send your CV, grade list and motivation letter (maximum of 1 A4 page) to myrthe.buser@radboudumc.nl and Alessa.Hering@radboudumc.nl .

## Suggested Reading

1.	A. Kutikov and R. G. Uzzo, “The R.E.N.A.L. nephrometry score: a comprehensive standardized system for quantitating renal tumor size, location and depth,” J. _Urol._, vol. 182, no. 3, pp. 844–853, Sep. 2009, doi: 10.1016/j.juro.2009.05.035.
2.	N. Abdallah _et al._, “AI-generated R.E.N.A.L.+ score surpasses human-generated score in predicting renal oncologic outcomes.,” _Urology_, vol. 180, pp. 160–167, Oct. 2023, doi: 10.1016/j.urology.2023.07.017.
3.	N. Heller _et al._, “Computer Generated R.E.N.A.L. Nephrometry Scores Yield Comparable Predictive Results to that of Human-Expert Scores in predicting oncologic and perioperative outcomes,” J. _Urol._, vol. 207, no. 5, pp. 1105–1115, May 2022, doi: 10.1097/JU.0000000000002390.


