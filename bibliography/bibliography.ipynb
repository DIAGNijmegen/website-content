{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import latexcodec\n",
    "import codecs\n",
    "import glob\n",
    "import os \n",
    "import latexcodec\n",
    "\n",
    "def save_dict2json(json_path, dict_md5):\n",
    "    with open(json_path, 'w') as fp:\n",
    "        json.dump(dict_md5, fp)\n",
    "\n",
    "def load_json2dict(json_path):\n",
    "    if os.path.exists(json_path):\n",
    "        json_file = open(json_path)\n",
    "        json_data = json.load(json_file)\n",
    "    else:\n",
    "        json_data = None\n",
    "    return json_data\n",
    "\n",
    "def timeit(method):\n",
    "    def timed(*args, **kw):\n",
    "        ts = time.time()\n",
    "        result = method(*args, **kw)\n",
    "        te = time.time()\n",
    "        if 'log_time' in kw:\n",
    "            name = kw.get('log_name', method.__name__.upper())\n",
    "            kw['log_time'][name] = int((te - ts) * 1000)\n",
    "        else:\n",
    "            print('%r ran for  %2.2f ms' % \\\n",
    "                  (method.__name__, (te - ts) * 1000))\n",
    "        return result\n",
    "    return timed\n",
    "\n",
    "name_mapping = {\n",
    "    \"Joeran S.\": \"J.S.\",\n",
    "}\n",
    "\n",
    "def authors_to_string(names, bib_key):\n",
    "    string_authors = ''\n",
    "    d = ', '\n",
    "    for idx, name in enumerate(names):\n",
    "        first, von, last, jr = name\n",
    "        if first:\n",
    "            if first in name_mapping:\n",
    "                first = name_mapping[first]\n",
    "            else:\n",
    "                first = first[0] + '.'\n",
    "        if idx == len(names)-2:\n",
    "            d = ' and '\n",
    "        if idx == len(names)-1:\n",
    "            d = ''\n",
    "        string_authors += ' '.join(part for part in [first, von, last, jr] if part) + d\n",
    "    return string_authors\n",
    "\n",
    "# pasre bib file\n",
    "def get_blocks(content, start_character='@', delim=('{','}')):\n",
    "    '''\n",
    "    returns all blocks (entries enclosed by the specified delimiters)\n",
    "    start_character will look backwards from the start of the block for this character\n",
    "    the result will be a tuple of two strings: from start character to start of the block, and the block content\n",
    "    '''\n",
    "    blocks = []\n",
    "    \n",
    "    delim_start, delim_end = delim\n",
    "    delimiter_stack = []\n",
    "    for i, c in enumerate(content):\n",
    "        if c == '{':\n",
    "            delimiter_stack.append(i)\n",
    "        elif c == '}' and delimiter_stack:\n",
    "            start = delimiter_stack.pop()\n",
    "            if len(delimiter_stack)==0:\n",
    "                start_index = content.rfind(start_character, 0, start)\n",
    "                blocks.append((content[start_index: start], content[start + 1: i]))\n",
    "    return blocks\n",
    "\n",
    "assert [x for x in get_blocks('abc = {test}, bac = {test2}', 'a')] == [('abc = ', 'test'), ('ac = ', 'test2')]\n",
    "\n",
    "\n",
    "def decode_name(name):\n",
    "    parsed_name = []\n",
    "    for name_part in name:\n",
    "        name_part = name_part.strip()\n",
    "        if '\\\\' in name_part:\n",
    "            name_part = codecs.decode(name_part, \"ulatex\")\n",
    "        parsed_name.append(name_part)\n",
    "    return parsed_name\n",
    "\n",
    "\n",
    "def parse_name(name):\n",
    "    '''\n",
    "    assumes this format:\n",
    "    https://tex.stackexchange.com/questions/557/how-should-i-type-author-names-in-a-bib-file\n",
    "    returns a tuple (first, von, last, jr)\n",
    "    '''\n",
    "    parts = name.strip(',').split(',')\n",
    "    \n",
    "    # \"First von Last\"\n",
    "    if len(parts)==1:\n",
    "        s, e = (name.index(' '), name.rfind(' ')) if ' ' in name else (0, 0)\n",
    "        first = name[:s]\n",
    "        von = name[s:e]\n",
    "        last = name[e:]\n",
    "        jr = ''\n",
    "        \n",
    "    # \"von Last, First\"\n",
    "    elif len(parts)==2: \n",
    "        first = parts[1]\n",
    "        e = parts[0].rfind(' ') if ' ' in parts[0] else 0\n",
    "        von = parts[0][:e]\n",
    "        last = parts[0][e:]\n",
    "        jr = ''\n",
    "        \n",
    "    # \"von Last, Jr, First\"\n",
    "    elif len(parts)==3: \n",
    "        first = parts[2]\n",
    "        e = parts[0].rfind(' ') if ' ' in parts[0] else 0\n",
    "        von = parts[0][:e]\n",
    "        last = parts[0][e:]\n",
    "        jr = parts[1]\n",
    "        \n",
    "    else:\n",
    "        print('warning! bibtex format error in name \"{}\"'.format(''.join(name)))\n",
    "        first, von, last, jr = '', '', name, ''  \n",
    "    \n",
    "    nfirst = ''\n",
    "    for f in first.strip().split('.'):\n",
    "        f = f.strip()\n",
    "        f = f.capitalize()\n",
    "        if len(f) == 1:\n",
    "            f += '.'    \n",
    "        nfirst = ' '.join([nfirst, f])\n",
    "    \n",
    "    #post process von to second names\n",
    "    nvon = ''\n",
    "    for v in von.strip().split():\n",
    "        v = v.strip()\n",
    "        if v[0].isupper():\n",
    "            nfirst = ' '.join([nfirst, v])\n",
    "        else:\n",
    "            nvon = ' '.join([nvon, v])\n",
    "    \n",
    "    #post process first and second names\n",
    "    nfirst2 = ''\n",
    "    for f in nfirst.strip().split():\n",
    "        f = f.strip()\n",
    "        f = f.capitalize()\n",
    "        if len(f) == 1:\n",
    "            f += '.'    \n",
    "        nfirst2 = ' '.join([nfirst2, f]) \n",
    "        \n",
    "    last = '-'.join(last.strip().split())\n",
    "        \n",
    "    return decode_name((nfirst2, nvon, last, jr))\n",
    "\n",
    "\n",
    "assert parse_name('Bart Liefers') == ['Bart', '', 'Liefers', '']\n",
    "assert parse_name('Bart von Liefers') == ['Bart', 'von', 'Liefers', '']\n",
    "assert parse_name('Liefers, Bart') == ['Bart', '', 'Liefers', '']\n",
    "assert parse_name('von Liefers, Bart') == ['Bart', 'von', 'Liefers', '']\n",
    "# assert parse_name('von Liefers, Jr, Bart') == ['Bart', 'von', 'Liefers', 'Jr']\n",
    "\n",
    "\n",
    "def parse_block_content(bib_item_text):\n",
    "    bib_item = {}\n",
    "    \n",
    "    # split lines and remove ',' at the end\n",
    "    bib_item_text = bib_item_text.replace('\\r', '\\n').replace('\\r\\n', '\\n')\n",
    "    lines  = bib_item_text.split(',\\n')\n",
    "    \n",
    "    # bib key\n",
    "    bib_key = lines[0].lower()\n",
    "    \n",
    "    # parse lines\n",
    "    for line in lines[1:]:\n",
    "        \n",
    "        # check if line is parseable\n",
    "        if '=' not in line or line == '':\n",
    "            continue \n",
    "            \n",
    "        # split by tags notated by '='   \n",
    "        key, value = line.split('=', 1)\n",
    "\n",
    "        # strip unneccary symbols\n",
    "        key = key.lower().strip()\n",
    "        value = value.strip().strip('{').strip('}')\n",
    "                    \n",
    "        # set bib_item\n",
    "        bib_item[key] = value\n",
    "    \n",
    "    return bib_key, bib_item\n",
    " \n",
    "def single_author(author_string):\n",
    "    splits = author_string.split(',')\n",
    "    splits = [s.strip() for s in splits]\n",
    "    return len(splits) == 2 and not(' ' in splits[0] and ' ' in splits[1])\n",
    "\n",
    "def split_authors(author_string):\n",
    "    authors = []\n",
    "    if 'and' in author_string.lower() or single_author(author_string):\n",
    "        authors = author_string.replace('AND', 'and').split(' and ')\n",
    "    else:\n",
    "        authors = author_string.split(',')\n",
    "    authors = [a.strip().replace('{', '').replace('}', '') for a in authors]\n",
    "    return authors\n",
    "\n",
    "def read_bibtex_file(filename):\n",
    "    bib_items = {}\n",
    "    string_rules = {}\n",
    "    with open(filename, 'rb') as f:\n",
    "        content = f.read().decode('utf-8-sig')\n",
    "\n",
    "    for block in get_blocks(content):\n",
    "        block_name, block_content = block\n",
    "        if block_name == '@Comment':\n",
    "            continue\n",
    "        elif block_name == '@String':\n",
    "            k, v = [x.strip() for x in block_content.split('=')]\n",
    "            string_rules[k] = v  \n",
    "        else: #bib item text\n",
    "\n",
    "            # parse bib item text\n",
    "            bib_key, bib_item = parse_block_content(block_content)\n",
    "            \n",
    "            # update type\n",
    "            bib_item['type'] = block_name.strip('@').lower()\n",
    "            \n",
    "            # update journal name\n",
    "            if 'journal' in bib_item:\n",
    "                bib_item['journal'] = string_rules[bib_item['journal']].strip('_').replace('_', ' ') if bib_item['journal'] in string_rules else bib_item['journal']\n",
    "\n",
    "            if 'author' in bib_item:\n",
    "                bib_item['author'] = list(map(parse_name, split_authors(bib_item['author'])))\n",
    "                bib_item['authors'] = authors_to_string(bib_item['author'], bib_key)\n",
    "\n",
    "            if 'abstract' in bib_item:\n",
    "                bib_item['abstract'] = bib_item['abstract'].replace('{', '').replace('}', '').replace('\\\\', '')\n",
    "                \n",
    "            if 'title' in bib_item:\n",
    "                bib_item['title'] = bib_item['title'].replace('{', '').replace('}', '').replace('\\\\', '')\n",
    "                                                                                                      \n",
    "            if 'copromotor' in bib_item:\n",
    "                try:\n",
    "                    bib_item['author'] += list(map(parse_name, split_authors(bib_item['copromotor'])))\n",
    "                except:\n",
    "                    print('bib_key cp', bib_key)\n",
    "           \n",
    "            if 'promotor' in bib_item:\n",
    "                try:\n",
    "                    bib_item['author'] += list(map(parse_name, split_authors(bib_item['promotor'])))\n",
    "                except:\n",
    "                    print('bib_key p', bib_key)\n",
    "\n",
    "            bib_items[bib_key.lower()] = bib_item\n",
    "    return bib_items\n",
    "\n",
    "\n",
    "# authors\n",
    "def get_list_researchers(members_path):\n",
    "    list_researchers = {}\n",
    "    for people_md_path in glob.glob(members_path + '/*.md'):\n",
    "        with open(people_md_path) as fp:\n",
    "            # parse md file\n",
    "            tags = {line.split(':')[0]:line.split(':')[1].strip().split() for line in (fp) if len(line.split(':')) > 1}\n",
    "\n",
    "            # get publication name\n",
    "            research_name = [n for n in tags['pub_name']] if 'pub_name' in tags else [n for n in tags['name']]\n",
    "            name = '-'.join(tags['name'])\n",
    "            groups =  [group.strip(',')  for group in tags['groups']]\n",
    "        # append researches \n",
    "        list_researchers[name] = (research_name, groups)\n",
    "    return list_researchers\n",
    "\n",
    "\n",
    "# author publications\n",
    "def get_publications_by_author(bib_items, list_researchers):\n",
    "    \"\"\"\n",
    "    Get all publication per author\n",
    "    returns dictionary with authorname as key and list of bib_keys as value\n",
    "    \"\"\"\n",
    "    author_bibkeys = {}\n",
    "    for bib_key, bib_item in bib_items.items():\n",
    "        if \"author\" not in bib_item:\n",
    "            continue\n",
    "        authors = bib_item[\"author\"]\n",
    "        for name, value in list_researchers.items():\n",
    "            researcher_name, _, _, _, _ = value\n",
    "            firstname = researcher_name[0].lower()\n",
    "            lastnames = [n.lower() for n in researcher_name[1:]]\n",
    "\n",
    "            if len(lastnames) > 1:\n",
    "                # This fixes issue #10 for lastnames connected with a dash (-)\n",
    "                lastnames.append(\"-\".join(lastnames))\n",
    "\n",
    "            for author_pub in authors:\n",
    "                if match_author_publication(firstname, lastnames, author_pub, bib_key):\n",
    "                    author_bibkeys.setdefault(name.lower(), []).append(bib_key)\n",
    "    return author_bibkeys\n",
    "\n",
    "\n",
    "def match_author_publication(firstname, lastnames, author, bib_key):\n",
    "    # This function selects authors with the same lastname and matches the first name.\n",
    "    # For instance, 'A. Patel' will always represent 'Ajay Patel' and not 'Anup Patel'.\n",
    "    # If the bib file contains 'A Patel' then, it will associate the bibentry to 'Ajay Patel'.\n",
    "    # If the bib file contains 'M F L Meijs' then, this script will not associate the bibentry to 'Midas Meijs'\n",
    "    #  because it will check for the existence of von and jr (F and L in the example)\n",
    "    author = [xname.replace('.', ' ').strip() for xname in author]\n",
    "    try:\n",
    "        first, von, last, jr = author\n",
    "    except:\n",
    "        print(author)\n",
    "    first = first.lower()\n",
    "    last = last.lower()\n",
    "    jr = jr.lower()\n",
    "    \n",
    "    # Additional variable that may help to avoid incorrect name matching #77\n",
    "    von_last = '-'.join([von, last])\n",
    "    von_last = von_last.replace(' ', '-').lower()\n",
    "\n",
    "    if last.lower() in lastnames:\n",
    "        # First match based on last name\n",
    "        if len(first) > 1 and first.lower() == firstname.lower() or len(jr) > 1 and jr.lower() in lastnames:\n",
    "            # Easy match, the first name is complete and matches up\n",
    "            return True\n",
    "        elif len(first) > 1 and ' ' in first:\n",
    "            # Incomplete match, some bib entries have authors as 'R Manniesing' instead of the full name\n",
    "            # or 'J A W M van der Laak' where firstname contains 'J A W M'\n",
    "            # or 'Jeroen AWM van der Laak' where firstname contains 'Jeroen A W M'\n",
    "            # This piece of code makes sure there is only one name and no spaces in between\n",
    "#             von = ' '.join(first.split(' ')[1:]) + ' ' + von\n",
    "            first = first.split(' ')[0].lower()\n",
    "            if first == firstname.lower():\n",
    "                return True\n",
    "            # if 'first' contains a single letter, it will continue\n",
    "\n",
    "        if len(first) == 1 and first[0].lower() == firstname[0].lower():\n",
    "            # If only one letter is provided as first name (incomplete in the bib entry).\n",
    "            # An additional variable stores the initial lastnames\n",
    "            initials_lastnames = [x[0].lower() for x in lastnames]\n",
    "            if (len(von) == 0 and len(jr) == 0):\n",
    "                # If there is no 'von' neither 'jr', then it is a match\n",
    "                return True\n",
    "            elif (len(jr) >= 1 and jr[0].lower() in initials_lastnames):\n",
    "                # If 'jr' contains something, it will have to be listed on 'initials_lastnames'\n",
    "                # to become a match\n",
    "                return True\n",
    "            elif (len(von.strip()) >= 1 and von.strip()[0].lower() in initials_lastnames):\n",
    "                # If 'von' contains something, it will have to be listed on 'initials_lastnames'\n",
    "                # to become a match\n",
    "                return True\n",
    "            elif '-' != von_last[0] and len(lastnames) >= 2 and lastnames[-1] in von_last:\n",
    "                # If none of the previous methods worked, an additional checkup is done.\n",
    "                # This is done only when having at least two last names.\n",
    "                # the last lastname should be in 'von_last'.\n",
    "                # For instance 'J A W M van der Laak' will become 'A-W-M-van-der-Laak', this matches up with 'van-der-laak'\n",
    "                return True\n",
    "        return False\n",
    "    else:\n",
    "        return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bart', 'von', 'Liefers', '']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_name('Bart von Liefers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get bib_items\n",
    "@timeit\n",
    "def parse_bib_file():\n",
    "    print('parsing bib file...')\n",
    "    bib_items = read_bibtex_file('../content/diag.bib')\n",
    "    \n",
    "    print('retreiving list of diag members')\n",
    "    list_researchers = get_list_researchers(r\"C:\\Users\\drepeeters\\PycharmProjects\\website-content\\content\\pages\\members\")  \n",
    "    \n",
    "    print('mapping bib keys to authors')\n",
    "    author_bib_keys = get_publications_by_author(bib_items, list_researchers)\n",
    "    \n",
    "    # print('saving bibitems.json')\n",
    "    # save_dict2json('/home/mart/bitems.json', bib_items)\n",
    "    \n",
    "    # print('saving auhtorbibkeys.json')\n",
    "    # save_dict2json('/home/mart/authorkeys.json', author_bib_keys)\n",
    "    \n",
    "    print('creating author md files')\n",
    "    print('creating publication md files')\n",
    "    return list_researchers, bib_items, author_bib_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_researchers, bib_items, author_bib_keys = parse_bib_file()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Stan', 'Constant', 'Richard', 'Noordman'], ['diag'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "researchers = get_list_researchers(r\"C:\\Users\\drepeeters\\PycharmProjects\\website-content\\content\\pages\\members\")\n",
    "researchers[\"Stan-Noordman\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'J.S. Bosma, A. Saha, M. Hosseinzadeh, I. Slootweg, M. de Rooij and H. Huisman'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_publications_by_author()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Grading prostate cancer is a time-consuming process and suffers from high inter- and intra-observer variability. Advances in computer-aided diagnosis have shown promise in improving histopathological diagnosis. We trained a deep learning system using data retrieved from the patients records to grade digitized prostate biopsies. Our system is the first that can automatically classify background, benign epithelium, Gleason 3, 4, and 5 on a gland-by-gland level in prostate biopsies. 532 glass slides containing 2162 prostate biopsies, evaluated by an experienced urogenital pathologist were collected and scanned. 596 biopsies were kept separate for evaluation, the remaining 1576 were used to train the deep learning algorithm (see table for Gleason grade distribution). A single label denoting the Gleason score (e.g. 3+4=7) was available for each biopsy, without information on tumor location or volume. To generate detailed annotations for training we used two previously trained deep learning networks to first segment the epithelium and, subsequently, to detect cancer. The Gleason grade from the patient record was assigned to the cancerous epithelium. These generated weakly annotated regions of tumor were then used to train a Gleason grading system. To evaluate, the system was applied to the biopsies in the test set. We used the total predicted surface area of each growth pattern to determine the Gleason score of the biopsy. Predicted tumor areas smaller than 15% of total epithelial tissue were considered unreliable (e.g. incomplete glands at the edges of the biopsy) and ignored for slide level classification. For predicted grades only areas larger than 5% of all epithelial tissue were considered, which is also common in clinical practice. Predicting whether a biopsy contains tumor resulted in an accuracy of 86% (linear weighted kappa (k) of 0.73, area under the ROC curve of 0.96). We compared the predicted primary Gleason grade to the one from the pathologists’ report. Our system achieved an accuracy of 75% (k 0.64). On predicting the Grade Group (using primary and secondary pattern), our system achieved an accuracy of 67% (k 0.57). Misclassifications of more than one grade are rare. Our deep learning system automatically identifies Gleason patterns and benign tissue on a gland-by-gland basis. This can be used to determine the biopsy-level Grade Group and Gleason score, and show which parts of the tissue contribute to this prediction. Improvements need to be made to decrease misclassifications, for example in areas with inflammation.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bib_items['bult19a']['abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assessment of tumor buds in colorectal cancer. A large-scale international digital observer study\n",
      "J. Bokhorst, H. Dawson, A. Blank, I. Zlobec, A. Lugli, M. Vieth, R. Kirsch, M. Urbanowicz, S. Brockmoeller, J. Flejou, L. Rijstenberg, J. van der Laak, F. Ciompi and I. Nagtegaal\n",
      "dict_keys(['author', 'title', 'booktitle', 'year', 'abstract', 'optnote', 'type', 'authors'])\n",
      "Learning from sparsely annotated data for semantic segmentation in histopathology images\n",
      "J. Bokhorst, H. Pinckaers, P. van Zwam, I. Nagetgaal, J. van der Laak and F. Ciompi\n",
      "dict_keys(['author', 'title', 'booktitle', 'year', 'volume', 'pages', 'url', 'abstract', 'file', 'optnote', 'type', 'authors'])\n",
      "Automatic Detection of Tumor Budding in Colorectal Carcinoma with Deep Learning\n",
      "J. Bokhorst, L. Rijstenberg, D. Goudkade, I. Nagtegaal, J. van der Laak and F. Ciompi\n",
      "dict_keys(['author', 'title', 'booktitle', 'year', 'date', 'publisher', 'month', 'doi', 'url', 'abstract', 'file', 'optnote', 'type', 'authors'])\n"
     ]
    }
   ],
   "source": [
    "for bkey in author_bib_keys['John-Melle-Bokhorst']: \n",
    "    print(bib_items[bkey]['title'])\n",
    "    print(bib_items[bkey]['authors'])\n",
    "    print(bib_items[bkey].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'author': [['Zaneta', '', 'Swiderska-Chadaj', ''],\n",
       "  ['Hans', '', 'Pinckaers', ''],\n",
       "  ['Mart', 'van', 'Rijthoven', ''],\n",
       "  ['Maschenka', '', 'Balkenhol', ''],\n",
       "  ['Margarita', '', 'Melnikova', ''],\n",
       "  ['Oscar', '', 'Geessink', ''],\n",
       "  ['Quirine', '', 'Manson', ''],\n",
       "  ['Geert', '', 'Litjens', ''],\n",
       "  ['Jeroen', 'van der', 'Laak', ''],\n",
       "  ['Francesco', '', 'Ciompi', '']],\n",
       " 'title': 'Convolutional Neural Networks for Lymphocyte detection in Immunohistochemically Stained Whole-Slide Images',\n",
       " 'booktitle': 'MIDL',\n",
       " 'year': '2018',\n",
       " 'url': 'https://openreview.net/forum?id=rk0xLisiM',\n",
       " 'abstract': 'Recent advances in cancer immunotherapy have boosted the interest in the role played by the immune system in cancer treatment. In particular, the presence of tumor-infiltrating lymphocytes (TILs) have become a central research topic in oncology and pathology. Consequently, a method to automatically detect and quantify immune cells is of great interest. In this paper, we present a comparison of different deep learning (DL) techniques for the detection of lymphocytes in immunohistochemically stained (CD3 and CD8) slides of breast, prostate and colon cancer. The compared methods cover the state-of-the-art in object localization, classification and segmentation: Locality Sensitive Method (LSM), U-net, You Only Look Once (YOLO) and fully-convolutional networks (FCNN). A dataset with 109,841 annotated cells from 58 whole-slide images was used for this study. Overall, U-net and YOLO achieved the highest results, with an F1-score of 0.78 in regular tissue areas. U-net approach was more robust to biological and staining variability and could also handle staining and tissue artifacts.',\n",
       " 'file': 'Swid18.pdf:pdf\\\\\\\\Swid18.pdf:PDF',\n",
       " 'optnote': 'DIAG',\n",
       " 'type': 'inproceedings',\n",
       " 'authors': 'Z. Swiderska-Chadaj, H. Pinckaers, M. van Rijthoven, M. Balkenhol, M. Melnikova, O. Geessink, Q. Manson, G. Litjens, J. van der Laak and F. Ciompi'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bib_items['swid18']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import latexcodec\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D. Abásolo, C. Gómez, J. Poza, M. García, C. Sánchez and M. López'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# author_bib_keys['clara-i-sánchez']\n",
    "codecs.decode(bib_items['abas05a']['authors'], \"ulatex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rijt18', 'swid18', 'swid19']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author_bib_keys['zaneta-swiderska-chadaj']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'101544'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bib_items['tell19']['pages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_md_file(output_path, md_content):\n",
    "    file = open(output_path, 'w', encoding='utf-8')\n",
    "    file.write(md_content)\n",
    "    file.close()\n",
    "    \n",
    "def create_author_md_files():\n",
    "    for name, bib_keys in author_bib_keys.items():\n",
    "        author_name = name.replace('_', ' ')\n",
    "        groups = list_researchers[name][1]\n",
    "        \n",
    "        md_string = 'title: Publications of ' + author_name.replace('-', ' ') + '\\n'\n",
    "        md_string += 'template: publications-author\\n'\n",
    "        md_string += 'author: ' + name +'\\n'\n",
    "        md_string += 'author_name: ' + author_name.replace('-', ' ') + '\\n'\n",
    "        md_string += 'groups: ' + ','.join(groups) + '\\n'\n",
    "        md_string += 'bibkeys: ' + ','.join(bib_keys)\n",
    "        md_file_name = name.lower() + '.md'\n",
    "        \n",
    "def create_publication_md():\n",
    "    for bib_key, bib_item in bib_items.items():\n",
    "        diag_authors = []\n",
    "        groups = set()\n",
    "        for name, bib_keys in author_bib_keys.items():\n",
    "            for bkey in bib_keys:\n",
    "                if bib_key == bkey:\n",
    "                    diag_authors.append(name)\n",
    "                    for group in list_researchers[name][1]:\n",
    "                        groups.add(group)\n",
    "                    md_string = 'title: ' + bib_item['title'] + '\\n'\n",
    "                    md_string += 'template: publication\\n'\n",
    "                    md_string += 'bibkey: ' + bib_key + '\\n'\n",
    "                    md_string += 'diag_authors: ' + ','.join(diag_authors) +'\\n'\n",
    "                    md_string += 'groups: ' + ','.join(groups) + '\\n'\n",
    "                    md_string += 'journal: NA \\n' if 'journal' not in bib_item else 'journal: ' + bib_item['journal'] + '\\n'\n",
    "                    md_string += 'year: NA \\n' if 'year' not in bib_item else 'year: ' + bib_item['year'] + '\\n'\n",
    "                    md_string += 'doi: NA \\n' if 'doi' not in bib_item else 'doi: ' + bib_item['doi'] + '\\n'\n",
    "                    md_string += 'url: NA \\n' if 'url' not in bib_item else 'url: ' + bib_item['url'] + '\\n'\n",
    "                    md_string += 'authors: ' + bib_item['authors'] + '\\n'\n",
    "                    md_string += 'has_pdf: True' + if 'file' in bib_item else 'has_pdf: False'\n",
    "                    md_string += '' if 'abstract' not in bib_item else bib_item['abstract']\n",
    "                    md_file_name = '/home/mart/Desktop/bib_files/' + bib_key + '.md'\n",
    "                    save_md_file(md_file_name, md_string)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title: Publications of Rashindra Manniesing\n",
      "template: publications-author\n",
      "author: Rashindra-Manniesing\n",
      "author_name: Rashindra Manniesing\n",
      "groups: diag,neuro\n",
      "bibkeys: deso19,avoi13,beck16,beck19a,boom14,boom12,boom12a,boom13,ciet11,ciet14,durm12,durm14,durm14a,eerd18,firo13,firo10a,firo10,firo12,firo12a,firo08,ghaf18,gish14,hame13,heuv16,koek12,koop08,koop08a,koop06,kort16,kort15,kosc18,kuij10,laue13,leem17,leem18c,leem18b,leem19b,leem18a,mann99a,mann06e,mann09a,mann09b,mann16a,mann00,mann99,mann04,mann98,mann10a,mann05a,mann07a,mann04a,mann06,mann06c,mann17,mann16,mann10,mann16c,mann04b,mann06a,mann08a,mann06b,mann07,mann09,mann09c,meij18c,meij18a,meij18,meij17a,meij18b,meij19a,meij17,meij16,mook09,mord13,mord12,oei13,oei14,oei14a,oei18,oei17,oei17a,oei12,pate17b,pate17a,pate19,pate16,pate18,pate17,pegg17a,roll09,scha07,stan18,tan16,leem19,leem19c,vuka12,vuka09a,vuka11,vuka08,vuka10,vuka09,wiel10,wiel11,pate19a\n"
     ]
    }
   ],
   "source": [
    "create_author_md_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bib_items = load_json2dict('/home/mart/radboudumc/website-content/content/bibitems.json')\n",
    "bib_author_keys = load_json2dict('/home/mart/radboudumc/website-content/content/authorkeys.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = b\"I{\\v{s}}gum\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "decoding with 'ulatex' codec failed (TypeError: cannot use a string pattern on a bytes-like object)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/latexcodec/codec.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, bytes_, errors)\u001b[0m\n\u001b[1;32m    835\u001b[0m         return (\n\u001b[0;32m--> 836\u001b[0;31m             \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    837\u001b[0m             \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/latexcodec/lexer.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, bytes_, final)\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34mu''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_unicode_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mUnicodeDecodeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/latexcodec/codec.py\u001b[0m in \u001b[0;36mget_unicode_tokens\u001b[0;34m(self, bytes_, final)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_unicode_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 788\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    789\u001b[0m             \u001b[0;31m# at this point, token_buffer does not match anything\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/latexcodec/lexer.py\u001b[0m in \u001b[0;36mget_tokens\u001b[0;34m(self, bytes_, final)\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_raw_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m             \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/latexcodec/lexer.py\u001b[0m in \u001b[0;36mget_raw_tokens\u001b[0;34m(self, bytes_, final)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_buffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memptytoken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m             \u001b[0;31m# yield the buffer token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot use a string pattern on a bytes-like object",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-6096a7d47677>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ulatex'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: decoding with 'ulatex' codec failed (TypeError: cannot use a string pattern on a bytes-like object)"
     ]
    }
   ],
   "source": [
    "a = codecs.decode(s, 'ulatex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I{\\x0b{s}}gum'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'author': [['Sil C.', 'van de', 'Leemput', ''],\n",
       "  ['Jonas', '', 'Teuwen', ''],\n",
       "  ['Bram', 'van', 'Ginneken', ''],\n",
       "  ['Rashindra', '', 'Manniesing', '']],\n",
       " 'title': 'MemCNN: A Python/PyTorch package for creating memory-efficient invertible neural networks',\n",
       " 'journal': '_Journal_of_Open_Source_Software_',\n",
       " 'year': '2019',\n",
       " 'date': '2019-07-30',\n",
       " 'volume': '4',\n",
       " 'number': '39',\n",
       " 'month': '7',\n",
       " 'pages': '1576',\n",
       " 'issn': '2475-9066',\n",
       " 'doi': 'https://doi.org/10.21105/joss.01576',\n",
       " 'url': 'http://dx.doi.org/10.21105/joss.01576',\n",
       " 'abstract': 'Neural networks are computational models that were originally inspired by biological neural networks like animal brains. These networks are composed of many small computational units called neurons that perform elementary calculations. Instead of explicitly programming the behavior of neural networks, these models can be trained to perform tasks, like classifying images, by presenting them examples. Sufficiently complex neural networks can automatically extract task-relevant characteristics from the presented examples without having prior knowledge about the task domain, which makes them attractive for many complicated real-world applications.   \\n\\n\\n\\nReversible operations have recently been successfully applied to classification problems to reduce memory requirements during neural network training. This feature is accomplished by removing the need to store the input activation for computing the gradients at the backward pass and instead reconstruct them on demand. However, current approaches rely on custom implementations of backpropagation, which limits applicability and extendibility. We present MemCNN, a novel PyTorch framework that simplifies the application of reversible functions by removing the need for a customized backpropagation. The framework contains a set of practical generalized tools, which can wrap common operations like convolutions and batch normalization and which take care of memory management. We validate the presented framework by reproducing state-of-the-art experiments using MemCNN and by comparing classification accuracy and training time on Cifar-10 and Cifar-100. Our MemCNN implementations achieved similar classification accuracy and faster training times while retaining compatibility with the default backpropagation facilities of PyTorch.',\n",
       " 'day': '30',\n",
       " 'file': ':pdf/Leem19b.pdf:PDF',\n",
       " 'optnote': 'RADIOLOGY,DIAG',\n",
       " 'publisher': 'The Open Journal',\n",
       " 'type': 'article',\n",
       " 'authors': 'S. van de Leemput, J. Teuwen, B. van Ginneken and R. Manniesing',\n",
       " 'coverpng': 'Leem19b.png',\n",
       " 'url_type': 'URL',\n",
       " 'cover_exists': 'False'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bib_items['leem19b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bib_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'sanc08b' in bib_author_keys['Clarisa-Sánchez']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = b\"S\\'anchez\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I{\\x0b{s}}gum'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.decode('latex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'\\\\\\'' in s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sánchez']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_name([s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"S'anchez\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codecs.decode(s, 'ulatex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "a bytes-like object is required, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-94e686320ee1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlatexcodec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcodec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutf_32_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: a bytes-like object is required, not 'str'"
     ]
    }
   ],
   "source": [
    "latexcodec.codec.codecs.utf_32_decode(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "bib_items = {}\n",
    "string_rules = {}\n",
    "with open('/home/mart/Downloads/fullstrings.bib', 'rb') as f:\n",
    "    content = f.read().decode('utf-8-sig')\n",
    "\n",
    "for block in get_blocks(content):\n",
    "    block_name, block_content = block\n",
    "    if '@String' in block_name:\n",
    "        k, v = [x.strip() for x in block_content.split('=')]\n",
    "        string_rules[k] = v "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'_Journal_of_Open_Source_Software_' in string_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_American_Journal_of_Transplantation_': '{American Journal of Transplantation}',\n",
       " '_Annals_of_the_American_Thoracic_Society_': '{}',\n",
       " '_Bulletin_of_the_Autralian_Mathematical_Society_': '{Bulletin of the Autralian Mathematical Society}',\n",
       " '_Computational_Pathology_and_Ophthalmic_Medical_Image_Analysis_': '{Computational Pathology and Ophthalmic Medical Image Analysis}',\n",
       " '_European_Congress_of_Pathology_': '{European Congress of Pathology}',\n",
       " '_European_Stroke_Journal_': '{European Stroke Journal}',\n",
       " '_Frontiers_in_ICT_': '{Frontiers in ICT}',\n",
       " '_GigaScience_': '{GigaScience}',\n",
       " '_Indagationes_Mathematicae_': '{Indagationes Mathematicae}',\n",
       " '_Infinite_Dimensional_Analysis_Quantum_Probability_and_Related_Topics_': '{Infinite Dimensional Analysis, Quantum Probability and Related Topics}',\n",
       " '_Interfase_Focus_': '{Interfase Focus}',\n",
       " '_International_Journal_of_Gynecological_Cancer_': '{International Journal of Gynaecological Cancer}',\n",
       " '_Journal_of_Invasive_Cardiology_': '{Journal of Invasive Cardiology}',\n",
       " '_Journal_of_the_Belgian_Society_of_Radiology_': '{Journal of the Belgian Society of Nephrology}',\n",
       " '_JoVE_': '{JoVE}',\n",
       " '_Nature_Biomedical_Engineering_': '{Nature Biomedical Engineering}',\n",
       " '_Nature_Immunology_': '{Nature Immunology}',\n",
       " '_Nature_Partner_Journals_Breast_Cancer_': '{NPJ Breast Cancer}',\n",
       " '_NeuroImage_Clinical_': '{NeuroImage: Clinical}',\n",
       " '_Oncotarget_': '{Oncotarget}',\n",
       " '_PeerJ_': '{PeerJ}',\n",
       " '_Public_Health_Action_': '{}',\n",
       " '_Theory_of_Computing_Systems_': '{Theory of Computing Systems}',\n",
       " '_Tijdschrift_voor_Urologie_': '{Tijdschrift voor Urologie}',\n",
       " '_World_Neurosurgery_': '{World Neurosurgery}'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key: Annals of the American Thoracic Society\n",
      "value: \n",
      "key: Infinite Dimensional Analysis Quantum Probability and Related Topics\n",
      "value: Infinite Dimensional Analysis, Quantum Probability and Related Topics\n",
      "key: International Journal of Gynecological Cancer\n",
      "value: International Journal of Gynaecological Cancer\n",
      "key: Journal of the Belgian Society of Radiology\n",
      "value: Journal of the Belgian Society of Nephrology\n",
      "key: Nature Partner Journals Breast Cancer\n",
      "value: NPJ Breast Cancer\n",
      "key: NeuroImage Clinical\n",
      "value: NeuroImage: Clinical\n",
      "key: Public Health Action\n",
      "value: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boollist = []\n",
    "\n",
    "for key, value  in string_rules.items():\n",
    "    key = key.replace('_', ' ').strip()\n",
    "    value = value.replace('}', '').replace('{', '')\n",
    "    if key != value:\n",
    "        print('key:', key)\n",
    "        print('value:', value)\n",
    "    boollist.append(key == value)\n",
    "    \n",
    "all(boollist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
